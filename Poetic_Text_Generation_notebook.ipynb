{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX4P1T-wLeoy"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WlxB1LkAL5_Y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# to create a neural network later on\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "#LSTM is the recurrent layer with memory\n",
        "#dense is the hidden layer\n",
        "#activation is the output layer\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Shakespeare file using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IQzh3MYnMcTa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#using tf to directly load the Shakespeare file into our script\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt',\n",
        "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()\n",
        "# we convert all the alphabets here to lower case to reduce the pool of characters from which the model has to predict one each time it has been asked to do so and hence reducing the computational power required.\n",
        "\n",
        "# text = text[20000:100000]\n",
        "# if you dont have time or high computational power, then you can train the model on a portion of the text by slicing it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iypCwVl1GKXQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "character = sorted(set(text))\n",
        "print(type(character))\n",
        "char_to_index = dict((c, i) for i, c in enumerate(character))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(character))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP7bVv0LGjvx"
      },
      "outputs": [],
      "source": [
        "SEQ_LENGTH = 40 #length of each sentence\n",
        "STEP_SIZE = 3 #steps moved before starting another sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM2aavSuGuaa"
      },
      "outputs": [],
      "source": [
        "sentences = [] #list of sentences\n",
        "next_char = [] #list of next characters for each sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_DRbc_fGz2Y"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "  sentences.append(text[i: i+SEQ_LENGTH])\n",
        "  next_char.append(text[i+SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing the arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eji9X6hUHVnm",
        "outputId": "5ec82cb2-f9a4-4afa-fee0-5b4c3a888d25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(character)), dtype = np.bool)  # initializing features array\n",
        "y = np.zeros((len(sentences), len(character)), dtype = np.bool)  # initializing labels array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filling up the features and label arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cixkWdXH1S2"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "  for j, char in enumerate(sentence):\n",
        "    x[i, j, char_to_index[char]] = 1\n",
        "  y[i, char_to_index[next_char[i]]] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXgS2dKXJgMD"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(character))))\n",
        "model.add(Dense(len(character)))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Q_ROo7Knt7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=RMSprop(lr=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLtZQS8bKBvd",
        "outputId": "37aa104c-d46e-4ee9-c046-032bd965d97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1453/1453 [==============================] - 252s 172ms/step - loss: 1.8828\n",
            "Epoch 2/4\n",
            "1453/1453 [==============================] - 247s 170ms/step - loss: 1.5626\n",
            "Epoch 3/4\n",
            "1453/1453 [==============================] - 245s 169ms/step - loss: 1.4927\n",
            "Epoch 4/4\n",
            "1453/1453 [==============================] - 242s 167ms/step - loss: 1.4567\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95587e62d0>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x, y, batch_size=256, epochs=4) #training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90mDt8SxKnXg"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    The function basically just picks one of the characters from the output. As parameters, it  takes the result of the prediction and a temperature. This temperature indicates how risky the pick shall be. If we have a high temperature, we will pick one of the less likely characters. A low temperature will cause a conservative choice.\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCW0xwP0LHNy"
      },
      "outputs": [],
      "source": [
        "def generate_text(length, temperature):\n",
        "  \"\"\"\n",
        "  It produces the final text of the provided length depending on the temperature.\n",
        "  \"\"\"\n",
        "  start_index = random.randint(0, len(text) - SEQ_LENGTH)\n",
        "  sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "  generated = \"\"\n",
        "  generated += sentence\n",
        "\n",
        "  for i in range(length):\n",
        "\n",
        "    x = np.zeros((1, SEQ_LENGTH, len(character)), dtype = np.bool)\n",
        "\n",
        "    for j, char in enumerate(sentence):\n",
        "      x[0, j, char_to_index[char]] = 1\n",
        "\n",
        "    x_predictions = model.predict(x, verbose=0)[0]\n",
        "\n",
        "    next_index = sample(x_predictions, temperature)\n",
        "    next_char = index_to_char[next_index]\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:] + next_char\n",
        "\n",
        "  return generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Printing out the texts generated by the program at various temperatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi9sM6-ZOGUX",
        "outputId": "903cc25e-25c5-4e04-db17-c654a8d5d609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------Temp 1--------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ther. thou\n",
            "shalt accompany us to the place,--will now,\n",
            "that my pawer.'say, mine paris, on? i have unlove of a will\n",
            "you gave this pation  hath a upon the\n",
            "eward's unglands weight wrench; linder a susgets,\n",
            "i will letten, bitwercoituo, and both beween shell;\n",
            "you be knows desires kates, accues elle my lord:\n",
            "why, sir, you do do good\n",
            "good so alminnance death and\n",
            "thy newss, to by thy soul: now, all yet.\n",
            "\n",
            "leontes:\n",
            "what he will make straight.\n",
            "\n",
            "friar laurence:\n",
            "like this;\n",
            "and standy leather: bind her art i take at no,\n",
            "being death of infocieve mea\n",
            "\n",
            "---------Temp 0.8--------\n",
            "right track of his fiery car,\n",
            "gives signior and way i, our thing, think purst\n",
            "i would gear my disores of fawness, and wark\n",
            "with gsathels, for fame him more than shall,\n",
            "i must be the heartfull callow--perceing; the did you,\n",
            "no what's thing stay again of thy queen's life\n",
            "him to will claudio that made since of us?\n",
            "\n",
            "tranio:\n",
            "\n",
            "o brother:\n",
            "your scoptier that our so shall quit adviled,\n",
            "stand him so, more neight, i must let them\n",
            "with anmornoned as pardon, my art the tridune;\n",
            "and let him to i promised, i have mean then.\n",
            "\n",
            "peorte:\n",
            "i long is botted\n",
            "\n",
            "---------Temp 0.6--------\n",
            " a soldier, as i lived a king.\n",
            "\n",
            "gloucester:\n",
            "and suit, all thy beathers in oward of this thing,\n",
            "which well love so mother, what, bear, distone;\n",
            "fast end these man and such sleeds which i crombth,\n",
            "and think that i'll gentleman, for you and follow your highness,\n",
            "which a streeps him of the stranger of the ears\n",
            "all the proud leave me straith: i am the strick,\n",
            "here all that think there would be so the god as nothing,\n",
            "for her forswork'd and have afternath, so sorrow\n",
            "in my brother be you, i will be so fley,\n",
            "that he state, young day of your lo\n",
            "\n",
            "---------Temp 0.4--------\n",
            ", there were crept,\n",
            "as 'twere in scorn of your will prove and heart.\n",
            "\n",
            "lord:\n",
            "i have a will be thy counselful hath beat\n",
            "that i have think thee which my down that be the last,\n",
            "my brother than the world be this stand of menion.\n",
            "\n",
            "duke vincentio:\n",
            "if the worth almer.\n",
            "\n",
            "provost:\n",
            "that he is heard that you not so the love\n",
            "of thy party mean that i have man nothing\n",
            "and be the man god with a the world so straight.\n",
            "\n",
            "provost:\n",
            "and lost the complain, sir, your complain,\n",
            "and the powering you are so strung of the business\n",
            "of a sword in the man i will not\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n---------Temp 1--------\")\n",
        "print(generate_text(500, 1))\n",
        "print(\"\\n---------Temp 0.8--------\")\n",
        "print(generate_text(500, 0.8))\n",
        "print(\"\\n---------Temp 0.6--------\")\n",
        "print(generate_text(500, 0.6))\n",
        "print(\"\\n---------Temp 0.4--------\")\n",
        "print(generate_text(500, 0.4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
